{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  \n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process word vectors with W2V with 1000000words:\n",
      "272.0413820743561\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "wordvectors_file_vec = '../../SBW-vectors-300-min5.txt'\n",
    "cantidad = 1000000\n",
    "start = time.time()\n",
    "print (\"Time to process word vectors with W2V with \"+str(cantidad)+\"words:\")\n",
    "\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process word vectors with fastText with 1000000 words:\n",
      "220.17412495613098\n"
     ]
    }
   ],
   "source": [
    "fastText_file_vec = '../../fasttext-sbwc.3.6.e20.vec'\n",
    "cantidad = 1000000\n",
    "start = time.time()\n",
    "print (\"Time to process word vectors with fastText with \"+str(cantidad)+\" words:\")\n",
    "\n",
    "fastTextVectors = KeyedVectors.load_word2vec_format(fastText_file_vec, limit=cantidad)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process word vectors with glove with 1000000 words:\n",
      "244.38113522529602\n"
     ]
    }
   ],
   "source": [
    "glove_file_vec = '../../glove-sbwc.i25.vec'\n",
    "cantidad = 1000000\n",
    "start = time.time()\n",
    "print (\"Time to process word vectors with glove with \"+str(cantidad)+\" words:\")\n",
    "\n",
    "glovevectors = KeyedVectors.load_word2vec_format(glove_file_vec, limit=cantidad)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palabra no esta en dict\n"
     ]
    }
   ],
   "source": [
    "#Testing word2vec\n",
    "try:\n",
    "    print(glovevectors.get_vector('hiperbole'))\n",
    "except:\n",
    "    print('palabra no esta en dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Ruizo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('spanish'))\n",
    "spanishStemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "#print(\"y\" in stopWords)\n",
    "#stemmer.stem(\"cordenada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hola', 'c칩moo', 'est치as']\n",
      "['hol', 'como', 'esta']\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lines= 'Hola c칩moo , est치as'\n",
    "words = Tokenizer.tokenize(lines)\n",
    "\n",
    "wordsStemmed = []\n",
    "for word in words:\n",
    "    word=spanishStemmer.stem(word)\n",
    "    wordsStemmed.append(word)\n",
    "print(words)\n",
    "print(wordsStemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process data:\n",
      "0.16097211837768555\n",
      "Cantidad de tweets \n",
      "60798\n"
     ]
    }
   ],
   "source": [
    "tree = ET.parse('general-tweets-test-tagged.xml')  \n",
    "root = tree.getroot()\n",
    "polarityNumberTest={}\n",
    "polarities=['NONE', 'NEU', 'P', 'N+', 'P+', 'N']\n",
    "tweetCount=0\n",
    "\n",
    "start = time.time()\n",
    "print (\"Time to process data:\")\n",
    "\n",
    "for tweet in root:\n",
    "    tweetId=tweet.find('tweetid').text\n",
    "    \n",
    "    tweetCount+=1\n",
    "    polarityNumber=0\n",
    "    polarity=tweet.find('sentiments').find('polarity').find('value').text\n",
    "    position=polarities.index(polarity)\n",
    "    polarityNumber+=position\n",
    "\n",
    "    polarityNumberTest[tweetId]=polarityNumber\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)\n",
    "print('Cantidad de tweets \\n'+ str(tweetCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process data:\n",
      "58.70062708854675\n",
      "Cantidad de tweets \n",
      "60798\n"
     ]
    }
   ],
   "source": [
    "tree = ET.parse('general-tweets-test.xml')  \n",
    "root = tree.getroot()\n",
    "tweetsTest=[]\n",
    "tweetCount=0\n",
    "\n",
    "start = time.time()\n",
    "print (\"Time to process data:\")\n",
    "\n",
    "for tweet in root:\n",
    "    wordsStemmed = []\n",
    "    polarityNumber=0\n",
    "    tweetText=tweet.find('content').text\n",
    "    tweetId=tweet.find('tweetid').text\n",
    "    \n",
    "    if tweetText and (tweetId in polarityNumberTest):\n",
    "        tweetCount+=1\n",
    "        words = Tokenizer.tokenize(tweetText)\n",
    "\n",
    "        for word in words:\n",
    "            word=spanishStemmer.stem(word)\n",
    "            if word not in stopWords:\n",
    "                wordsStemmed.append(word)\n",
    "\n",
    "        newEntry=[wordsStemmed,polarityNumberTest[tweetId]]\n",
    "\n",
    "        tweetsTest.append(newEntry)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)\n",
    "print('Cantidad de tweets \\n'+ str(tweetCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process data:\n",
      "6.762438058853149\n",
      "Output as [[[ListOfWords],[polarityNumber]],[[ListOfWords],[polarityNumber]],...]\n",
      "Cantidad de tweets \n",
      "7219\n"
     ]
    }
   ],
   "source": [
    "tree = ET.parse('general-tweets-train-tagged.xml')  \n",
    "root = tree.getroot()\n",
    "tweets=[]\n",
    "polarities=['NONE', 'NEU', 'P', 'N+', 'P+', 'N']\n",
    "polarityTypes=['AGREEMENT', 'DISAGREEMENT']\n",
    "tweetCount=0\n",
    "\n",
    "start = time.time()\n",
    "print (\"Time to process data:\")\n",
    "\n",
    "for tweet in root:\n",
    "    tweetCount+=1\n",
    "    wordsStemmed = []\n",
    "    polarityNumber=0\n",
    "    tweetText=tweet.find('content').text\n",
    "    \n",
    "    if tweetText:\n",
    "        words = Tokenizer.tokenize(tweetText)\n",
    "\n",
    "        for word in words:\n",
    "            word=spanishStemmer.stem(word)\n",
    "            if word not in stopWords:\n",
    "                wordsStemmed.append(word)\n",
    "\n",
    "        polarity=tweet.find('sentiments').find('polarity').find('value').text\n",
    "        position=polarities.index(polarity)\n",
    "        polarityNumber+=position\n",
    "\n",
    "        #polarityType=tweet.find('sentiments').find('polarity').find('type').text\n",
    "        #position=polarityTypes.index(polarityType)\n",
    "        #polarityNumber+=6*position\n",
    "\n",
    "        newEntry=[wordsStemmed,polarityNumber]\n",
    "\n",
    "        tweets.append(newEntry)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)\n",
    "print (\"Output as [[[ListOfWords],[polarityNumber]],[[ListOfWords],[polarityNumber]],...]\")\n",
    "print('Cantidad de tweets \\n'+ str(tweetCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to vectorize tweets:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ruizo/.pyenv/versions/3.4.6/lib/python3.4/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/Ruizo/.pyenv/versions/3.4.6/lib/python3.4/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.540654182434082\n"
     ]
    }
   ],
   "source": [
    "vectorizedPolarityInfo=[]\n",
    "\n",
    "#Word2Vect\n",
    "vectorizedTweetsInfoW2V=[]\n",
    "\n",
    "#FastText\n",
    "vectorizedTweetsInfoFT=[]\n",
    "\n",
    "#Glove\n",
    "vectorizedTweetsInfoG=[]\n",
    "\n",
    "start = time.time()\n",
    "print(\"Time to vectorize tweets:\")\n",
    "\n",
    "for tweet in tweets:\n",
    "    vectorizedTweetsW2V=[]\n",
    "    vectorizedTweetsFT=[]\n",
    "    vectorizedTweetsG=[]\n",
    "    for word in tweet[0]:\n",
    "        try:\n",
    "            vectorW2V = wordvectors.get_vector(word)\n",
    "            vectorizedTweetsW2V.append(vectorW2V)\n",
    "            \n",
    "            vectorFT = fastTextVectors.get_vector(word)\n",
    "            vectorizedTweetsFT.append(vectorFT)\n",
    "            \n",
    "            vectorG = glovevectors.get_vector(word)\n",
    "            vectorizedTweetsG.append(vectorG)\n",
    "        except:\n",
    "            pass    \n",
    "    vectorMeanW2V = np.mean(np.array(vectorizedTweetsW2V),axis=0)\n",
    "    vectorMeanFT = np.mean(np.array(vectorizedTweetsFT),axis=0)\n",
    "    vectorMeanG = np.mean(np.array(vectorizedTweetsG),axis=0)\n",
    "    try:\n",
    "        len(vectorMeanW2V)\n",
    "        vectorizedTweetsInfoW2V.append(vectorMeanW2V.tolist())        \n",
    "        vectorizedTweetsInfoFT.append(vectorMeanFT.tolist())        \n",
    "        vectorizedTweetsInfoG.append(vectorMeanG.tolist())\n",
    "        \n",
    "        vectorizedPolarityInfo.append(tweet[1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to vectorize tweets:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ruizo/.pyenv/versions/3.4.6/lib/python3.4/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/Ruizo/.pyenv/versions/3.4.6/lib/python3.4/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.609647035598755\n"
     ]
    }
   ],
   "source": [
    "vectorizedPolarityTestInfo=[]\n",
    "\n",
    "#Word2Vect\n",
    "vectorizedTweetsInfoTestW2V=[]\n",
    "\n",
    "#FastText\n",
    "vectorizedTweetsInfoTestFT=[]\n",
    "\n",
    "#Glove\n",
    "vectorizedTweetsInfoTestG=[]\n",
    "\n",
    "start = time.time()\n",
    "print(\"Time to vectorize tweets:\")\n",
    "\n",
    "for tweet in tweetsTest:\n",
    "    vectorizedTweetsTestW2V=[]\n",
    "    vectorizedTweetsTestFT=[]\n",
    "    vectorizedTweetsTestG=[]\n",
    "    for word in tweet[0]:\n",
    "        try:\n",
    "            vectorW2V = wordvectors.get_vector(word)\n",
    "            vectorizedTweetsTestW2V.append(vectorW2V)\n",
    "            \n",
    "            vectorFT = fastTextVectors.get_vector(word)\n",
    "            vectorizedTweetsTestFT.append(vectorFT)\n",
    "            \n",
    "            vectorG = glovevectors.get_vector(word)\n",
    "            vectorizedTweetsTestG.append(vectorG)\n",
    "        except:\n",
    "            pass    \n",
    "    vectorMeanTestW2V = np.mean(np.array(vectorizedTweetsTestW2V),axis=0)\n",
    "    vectorMeanTestFT = np.mean(np.array(vectorizedTweetsTestFT),axis=0)\n",
    "    vectorMeanTestG = np.mean(np.array(vectorizedTweetsTestG),axis=0)\n",
    "    try:\n",
    "        len(vectorMeanTestW2V)\n",
    "        vectorizedTweetsInfoTestW2V.append(vectorMeanTestW2V.tolist())        \n",
    "        vectorizedTweetsInfoTestFT.append(vectorMeanTestFT.tolist())        \n",
    "        vectorizedTweetsInfoTestG.append(vectorMeanTestG.tolist())\n",
    "        \n",
    "        vectorizedPolarityTestInfo.append(tweet[1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "#Entrenar\n",
    "svm_W2V = svm.SVC(kernel='linear', C=1)\n",
    "svm_FT = svm.SVC(kernel='linear', C=1)\n",
    "svm_G = svm.SVC(kernel='linear', C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model with W2V:\n",
      "23.493000984191895\n",
      "Time to train model with FastText:\n",
      "21.531985998153687\n",
      "Time to train model with Glove:\n",
      "24.265070915222168\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "print (\"Time to train model with W2V:\")\n",
    "svm_W2V.fit(vectorizedTweetsInfoW2V,vectorizedPolarityInfo)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "print (\"Time to train model with FastText:\")\n",
    "svm_FT.fit(vectorizedTweetsInfoFT, vectorizedPolarityInfo)\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "\n",
    "start = time.time()\n",
    "print (\"Time to train model with Glove:\")\n",
    "svm_G.fit(vectorizedTweetsInfoG, vectorizedPolarityInfo)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Accuracy W2V: 0.371364653244\n",
      "Test Set Accuracy W2V: 0.470426455482\n",
      "Training Set Accuracy FastText: 0.479166666667\n",
      "Test Set Accuracy FastText: 0.451224361311\n",
      "Training Set Accuracy Glove: 0.470078299776\n",
      "Test Set Accuracy Glove: 0.425875608867\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set Accuracy W2V:\",svm_W2V.score(vectorizedTweetsInfoW2V,vectorizedPolarityInfo))\n",
    "print(\"Test Set Accuracy W2V:\",svm_W2V.score(vectorizedTweetsInfoTestW2V,vectorizedPolarityTestInfo))\n",
    "\n",
    "print(\"Training Set Accuracy FastText:\",svm_FT.score(vectorizedTweetsInfoFT,vectorizedPolarityInfo))\n",
    "print(\"Test Set Accuracy FastText:\",svm_FT.score(vectorizedTweetsInfoTestFT,vectorizedPolarityTestInfo))\n",
    "\n",
    "print(\"Training Set Accuracy Glove:\",svm_G.score(vectorizedTweetsInfoG,vectorizedPolarityInfo))\n",
    "print(\"Test Set Accuracy Glove:\",svm_G.score(vectorizedTweetsInfoTestG,vectorizedPolarityTestInfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confussion matrix W2V:\n",
      " [[ 9459     0   229    15  4966  6450]\n",
      " [  118     0    11     1   379   796]\n",
      " [  372     0    36     0   631   446]\n",
      " [  673     0    50    21   784  3026]\n",
      " [ 4536     0   155     5 11910  4023]\n",
      " [ 2223     0   107    11  1957  6968]]\n",
      "\n",
      "Confussion matrix FastText:\n",
      " [[ 9326   657  2272  1063  3709  4092]\n",
      " [  100    94   177   142   260   532]\n",
      " [  246    44   643    37   296   219]\n",
      " [  482   130   327  1684   438  1493]\n",
      " [ 4133   540  2888   566 10187  2315]\n",
      " [ 1960   490  1135  1179  1201  5301]]\n",
      "\n",
      "Confussion matrix Glove:\n",
      " [[8919  823 2661 1367 3341 4008]\n",
      " [ 110  136  183  162  241  473]\n",
      " [ 220   69  653   53  273  217]\n",
      " [ 538  169  431 1537  482 1397]\n",
      " [4146  757 3032  899 9398 2397]\n",
      " [1925  559 1201 1263 1256 5062]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print ('\\nConfussion matrix W2V:\\n',confusion_matrix(vectorizedPolarityTestInfo, svm_W2V.predict(vectorizedTweetsInfoTestW2V)))\n",
    "print ('\\nConfussion matrix FastText:\\n',confusion_matrix(vectorizedPolarityTestInfo, svm_FT.predict(vectorizedTweetsInfoTestFT)))\n",
    "print ('\\nConfussion matrix Glove:\\n',confusion_matrix(vectorizedPolarityTestInfo, svm_G.predict(vectorizedTweetsInfoTestG)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
