{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET  \n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process word vectors with W2V with 10000000words:\n",
      "276.2462651729584\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "wordvectors_file_vec = '../../SBW-vectors-300-min5.txt'\n",
    "cantidad = 10000000\n",
    "start = time.time()\n",
    "print (\"Time to process word vectors with W2V with \"+str(cantidad)+\"words:\")\n",
    "\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process word vectors with fastText with 10000000 words:\n",
      "243.37566304206848\n"
     ]
    }
   ],
   "source": [
    "fastText_file_vec = '../../fasttext-sbwc.3.6.e20.vec'\n",
    "cantidad = 10000000\n",
    "start = time.time()\n",
    "print (\"Time to process word vectors with fastText with \"+str(cantidad)+\" words:\")\n",
    "\n",
    "fastTextVectors = KeyedVectors.load_word2vec_format(fastText_file_vec, limit=cantidad)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process word vectors with glove with 10000000 words:\n",
      "240.00987815856934\n"
     ]
    }
   ],
   "source": [
    "glove_file_vec = '../../glove-sbwc.i25.vec'\n",
    "cantidad = 10000000\n",
    "start = time.time()\n",
    "print (\"Time to process word vectors with glove with \"+str(cantidad)+\" words:\")\n",
    "\n",
    "glovevectors = KeyedVectors.load_word2vec_format(glove_file_vec, limit=cantidad)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palabra no esta en dict\n"
     ]
    }
   ],
   "source": [
    "#Testing word2vec\n",
    "try:\n",
    "    print(glovevectors.get_vector('hiperbole'))\n",
    "except:\n",
    "    print('palabra no esta en dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Ruizo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('spanish'))\n",
    "spanishStemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "#print(\"y\" in stopWords)\n",
    "#stemmer.stem(\"cordenada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'c칩moo', 'est치as']\n",
      "['hol', 'como', 'esta']\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = RegexpTokenizer(r'\\w+')\n",
    "lines= 'hola c칩moo , est치as'\n",
    "words = Tokenizer.tokenize(lines)\n",
    "\n",
    "wordsStemmed = []\n",
    "for word in words:\n",
    "    word=spanishStemmer.stem(word)\n",
    "    wordsStemmed.append(word)\n",
    "print(words)\n",
    "print(wordsStemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to process data:\n",
      "6.259454965591431\n",
      "Output as [[[ListOfWords],[polarityNumber]],[[ListOfWords],[polarityNumber]],...]\n",
      "Cantidad de tweets \n",
      "7219\n"
     ]
    }
   ],
   "source": [
    "tree = ET.parse('general-tweets-train-tagged.xml')  \n",
    "root = tree.getroot()\n",
    "tweets=[]\n",
    "polarities=['NONE', 'NEU', 'P', 'N+', 'P+', 'N']\n",
    "polarityTypes=['AGREEMENT', 'DISAGREEMENT']\n",
    "tweetCount=0\n",
    "\n",
    "start = time.time()\n",
    "print (\"Time to process data:\")\n",
    "\n",
    "for tweet in root:\n",
    "    tweetCount+=1\n",
    "    wordsStemmed = []\n",
    "    polarityNumber=0\n",
    "    tweetText=tweet.find('content').text\n",
    "    \n",
    "    if tweetText:\n",
    "        words = Tokenizer.tokenize(tweetText)\n",
    "\n",
    "        for word in words:\n",
    "            word=spanishStemmer.stem(word)\n",
    "            if word not in stopWords:\n",
    "                wordsStemmed.append(word)\n",
    "\n",
    "        polarity=tweet.find('sentiments').find('polarity').find('value').text\n",
    "        position=polarities.index(polarity)\n",
    "        polarityNumber+=position\n",
    "\n",
    "        polarityType=tweet.find('sentiments').find('polarity').find('type').text\n",
    "        position=polarityTypes.index(polarityType)\n",
    "        polarityNumber+=6*position\n",
    "\n",
    "        newEntry=[wordsStemmed,polarityNumber]\n",
    "\n",
    "        tweets.append(newEntry)\n",
    "\n",
    "end = time.time()\n",
    "print (end - start)\n",
    "print (\"Output as [[[ListOfWords],[polarityNumber]],[[ListOfWords],[polarityNumber]],...]\")\n",
    "print('Cantidad de tweets \\n'+ str(tweetCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to vectorize tweets:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ruizo/.pyenv/versions/3.4.6/lib/python3.4/site-packages/numpy/core/fromnumeric.py:2889: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/Ruizo/.pyenv/versions/3.4.6/lib/python3.4/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8740031719207764\n"
     ]
    }
   ],
   "source": [
    "#Word2Vect\n",
    "vectorizedTweetsInfoW2V=[]\n",
    "vectorizedPolarityInfo=[]\n",
    "\n",
    "#FastText\n",
    "vectorizedTweetsInfoFT=[]\n",
    "\n",
    "#Glove\n",
    "vectorizedTweetsInfoG=[]\n",
    "\n",
    "start = time.time()\n",
    "print(\"Time to vectorize tweets:\")\n",
    "\n",
    "for tweet in tweets:\n",
    "    vectorizedTweetsW2V=[]\n",
    "    vectorizedTweetsFT=[]\n",
    "    vectorizedTweetsG=[]\n",
    "    for word in tweet[0]:\n",
    "        try:\n",
    "            vectorW2V = wordvectors.get_vector(word)\n",
    "            vectorizedTweetsW2V.append(vectorW2V)\n",
    "            \n",
    "            vectorFT = fastTextVectors.get_vector(word)\n",
    "            vectorizedTweetsFT.append(vectorFT)\n",
    "            \n",
    "            vectorG = glovevectors.get_vector(word)\n",
    "            vectorizedTweetsG.append(vectorG)\n",
    "        except:\n",
    "            pass    \n",
    "    vectorMeanW2V = np.mean(np.array(vectorizedTweetsW2V),axis=0)\n",
    "    vectorMeanFT = np.mean(np.array(vectorizedTweetsFT),axis=0)\n",
    "    vectorMeanG = np.mean(np.array(vectorizedTweetsG),axis=0)\n",
    "    try:\n",
    "        len(vectorMeanW2V)\n",
    "        vectorizedTweetsInfoW2V.append(vectorMeanW2V.tolist())        \n",
    "        vectorizedTweetsInfoFT.append(vectorMeanFT.tolist())        \n",
    "        vectorizedTweetsInfoG.append(vectorMeanG.tolist())\n",
    "        \n",
    "        vectorizedPolarityInfo.append(tweet[1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7065\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizedTweetsInfo))\n",
    "#for val in vectorizedPolarityInfo:\n",
    "#    print (val)\n",
    "#print(len(vectorizedPolarityInfo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Entrenar\n",
    "clf = svm.SVC(kernel='linear', C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model with W2V:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ruizo/.pyenv/versions/3.4.6/lib/python3.4/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.3033241   0.30833333  0.32684284  0.36211699  0.35244755  0.33099579\n",
      "  0.34831461  0.35302391  0.32489451  0.34739803] 0.335769166929\n",
      "210.39200901985168\n"
     ]
    }
   ],
   "source": [
    "#W2V\n",
    "start = time.time()\n",
    "print (\"Time to train model with W2V:\")\n",
    "    \n",
    "scoresW2V = cross_val_score(clf, vectorizedTweetsInfoW2V, vectorizedPolarityInfo, cv=10)\n",
    "print(scoresW2V, sum(scoresW2V)/len(scoresW2V))\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model with FastText:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ruizo/.pyenv/versions/3.4.6/lib/python3.4/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.31855956  0.31805556  0.3449235   0.34679666  0.35244755  0.38569425\n",
      "  0.38483146  0.37271449  0.36990155  0.33192686] 0.352585143469\n",
      "198.28459882736206\n"
     ]
    }
   ],
   "source": [
    "#FT\n",
    "start = time.time()\n",
    "print (\"Time to train model with FastText:\")\n",
    "    \n",
    "scoresFT = cross_val_score(clf, vectorizedTweetsInfoFT, vectorizedPolarityInfo, cv=10)\n",
    "print(scoresFT, sum(scoresFT)/len(scoresFT))\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model with Glove:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ruizo/.pyenv/versions/3.4.6/lib/python3.4/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 4 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33379501  0.30694444  0.32127955  0.3551532   0.37762238  0.35063114\n",
      "  0.35533708  0.34317862  0.33895921  0.3347398 ] 0.341764044602\n",
      "222.26583313941956\n"
     ]
    }
   ],
   "source": [
    "#G\n",
    "start = time.time()\n",
    "print (\"Time to train model with Glove:\")\n",
    "    \n",
    "scoresG = cross_val_score(clf, vectorizedTweetsInfoG, vectorizedPolarityInfo, cv=10)\n",
    "print(scoresG, sum(scoresG)/len(scoresG))\n",
    "\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
