{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import operator\n",
    "import gensim \n",
    "import numpy as np\n",
    "from stemming.porter2 import stem\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/feliperojos/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "31749\n"
     ]
    }
   ],
   "source": [
    "dictionary = {}\n",
    "dictionary_index = {}\n",
    "exp_path_pos = '../review_polarity/txt_sentoken/pos/'\n",
    "exp_path_neg = '../review_polarity/txt_sentoken/neg/'\n",
    "nltk.download('stopwords')\n",
    "stopWords = set(stopwords.words('english'))\n",
    "stopW_without_punctuation = []\n",
    "\n",
    "documents = list()\n",
    "\n",
    "for i in stopWords:\n",
    "    stopW_without_punctuation.append(re.sub(r'[^\\w\\s]','',i))\n",
    "\n",
    "for filename in os.listdir(exp_path_pos):\n",
    "    with open(exp_path_pos+filename,'r') as f:\n",
    "        for line in f:\n",
    "            #delete Punctuation\n",
    "            line = re.sub(r'[^\\w\\s]','',line)\n",
    "            tempLine = \"\"\n",
    "            for word in line.split():\n",
    "                word = word.lower()\n",
    "                #Stop Words\n",
    "                if word not in stopW_without_punctuation:\n",
    "                    temp_w = stem(word)\n",
    "                    tempLine += temp_w + \" \"\n",
    "                    if temp_w in dictionary:\n",
    "                        dictionary[temp_w]=dictionary[temp_w] + 1\n",
    "                    else:\n",
    "                        dictionary[temp_w]=1\n",
    "                    \n",
    "            tempLine = tempLine.strip()\n",
    "            #El preprocesado simple que trae genism elimina palabras, por lo que cambia el diccionario, lo que hace menos objetiva la comparación    \n",
    "            #documents.append(gensim.utils.simple_preprocess(tempLine))\n",
    "            documents.append(tempLine.split())\n",
    "            \n",
    "for filename in os.listdir(exp_path_neg):\n",
    "    with open(exp_path_neg+filename,'r') as f:\n",
    "        for line in f:\n",
    "            #delete Punctuation\n",
    "            line = re.sub(r'[^\\w\\s]','',line)\n",
    "            tempLine = \"\"\n",
    "            for word in line.split():\n",
    "                word = word.lower()\n",
    "                #Stop Words\n",
    "                if word not in stopW_without_punctuation:\n",
    "                    temp_w = stem(word)\n",
    "                    tempLine += temp_w + \" \"\n",
    "                    if temp_w in dictionary:\n",
    "                        dictionary[temp_w]=dictionary[temp_w] + 1\n",
    "                    else:\n",
    "                        dictionary[temp_w]=1\n",
    "\n",
    "            tempLine = tempLine.strip()        \n",
    "           \n",
    "            #El preprocesado simple que trae genism elimina palabras, por lo que cambia el diccionario, lo que hace menos objetiva la comparación    \n",
    "            #documents.append(gensim.utils.simple_preprocess(tempLine))\n",
    "            documents.append(tempLine.split())\n",
    "\n",
    "print(len(dictionary))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('santostefano', 1)\n",
      "('alin', 1)\n",
      "('brosh', 1)\n",
      "('romanticmistaken', 1)\n",
      "('bulimia', 1)\n",
      "('film', 11108)\n",
      "('movi', 6855)\n",
      "('one', 5758)\n",
      "('like', 3997)\n",
      "('charact', 3855)\n"
     ]
    }
   ],
   "source": [
    "# Mas frecuentes y menos frecuentes\n",
    "sorted_dictionary = sorted(dictionary.items(), key=operator.itemgetter(1))\n",
    "print(sorted_dictionary[0])\n",
    "print(sorted_dictionary[1])\n",
    "print(sorted_dictionary[2])\n",
    "print(sorted_dictionary[3])\n",
    "print(sorted_dictionary[4])\n",
    "\n",
    "print(sorted_dictionary[-1])\n",
    "print(sorted_dictionary[-2])\n",
    "print(sorted_dictionary[-3])\n",
    "print(sorted_dictionary[-4])\n",
    "print(sorted_dictionary[-5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparando para BoW\n",
    "dictKeys = list(dictionary.keys())\n",
    "for i in range(len(dictKeys)):\n",
    "    dictionary_index[dictKeys[i]]=i\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6663263, 7034960)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#codificacion BOW y word2vector\n",
    "bow_cods = []\n",
    "w2v_cods = []\n",
    "labels = []\n",
    "model = gensim.models.Word2Vec (documents, size=100, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(exp_path_pos):\n",
    "    with open(exp_path_pos+filename,'r') as f:\n",
    "        bow_temp = [0]*len(dictionary)\n",
    "        w2v_temp = np.empty((0,100))\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            #delete Punctuation\n",
    "            line = re.sub(r'[^\\w\\s]','',line)\n",
    "            tempLine = \"\"\n",
    "            for word in line.split():\n",
    "                word = word.lower()\n",
    "                #Stop Words\n",
    "                if word not in stopW_without_punctuation:\n",
    "                    temp_w = stem(word)\n",
    "                    bow_temp[dictionary_index[temp_w]]+=1\n",
    "                    if (temp_w in model.wv.vocab):\n",
    "                        w2v_resize = np.array(model.wv[temp_w])\n",
    "                        w2v_resize = np.resize(w2v_resize,(1,100))\n",
    "                        w2v_temp = np.append(w2v_temp,w2v_resize, axis=0)\n",
    "        w2v_temp = np.mean(w2v_temp, axis=0)\n",
    "        bow_cods.append([bow_temp, \"pos\"])\n",
    "        w2v_cods.append([w2v_temp, \"pos\"])\n",
    "        \n",
    "for filename in os.listdir(exp_path_neg):\n",
    "    with open(exp_path_neg+filename,'r') as f:\n",
    "        bow_temp = [0]*len(dictionary)\n",
    "        w2v_temp = np.empty((0,100))\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            #delete Punctuation\n",
    "            line = re.sub(r'[^\\w\\s]','',line)\n",
    "            tempLine = \"\"\n",
    "            for word in line.split():\n",
    "                word = word.lower()\n",
    "                #Stop Words\n",
    "                if word not in stopW_without_punctuation:\n",
    "                    temp_w = stem(word)\n",
    "                    bow_temp[dictionary_index[temp_w]]+=1\n",
    "                    if (temp_w in model.wv.vocab):\n",
    "                        w2v_resize = np.array(model.wv[temp_w])\n",
    "                        w2v_resize = np.resize(w2v_resize,(1,100))\n",
    "                        w2v_temp = np.append(w2v_temp,w2v_resize, axis=0)\n",
    "        w2v_temp = np.mean(w2v_temp, axis=0)\n",
    "        bow_cods.append([bow_temp, \"neg\"])\n",
    "        w2v_cods.append([w2v_temp, \"neg\"])\n",
    "        \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración numero 1:\n",
      "\n",
      "Training Set Accuracy BOW: 1.0\n",
      "Testing Set Accuracy BOW: 0.765\n",
      "Training Set Accuracy W2V: 0.799375\n",
      "Testing Set Accuracy W2V: 0.6425\n",
      "\n",
      "\n",
      "Confussion matrix BOW:\n",
      " [[  0   0]\n",
      " [ 94 306]]\n",
      "\n",
      "Confussion matrix W2V:\n",
      " [[  0   0]\n",
      " [143 257]]\n",
      "\n",
      "Iteración numero 2:\n",
      "\n",
      "Training Set Accuracy BOW: 1.0\n",
      "Testing Set Accuracy BOW: 0.7625\n",
      "Training Set Accuracy W2V: 0.810625\n",
      "Testing Set Accuracy W2V: 0.6075\n",
      "\n",
      "\n",
      "Confussion matrix BOW:\n",
      " [[  0   0]\n",
      " [ 95 305]]\n",
      "\n",
      "Confussion matrix W2V:\n",
      " [[  0   0]\n",
      " [157 243]]\n",
      "\n",
      "Iteración numero 3:\n",
      "\n",
      "Training Set Accuracy BOW: 1.0\n",
      "Testing Set Accuracy BOW: 0.85\n",
      "Training Set Accuracy W2V: 0.786875\n",
      "Testing Set Accuracy W2V: 0.7725\n",
      "\n",
      "\n",
      "Confussion matrix BOW:\n",
      " [[178  22]\n",
      " [ 38 162]]\n",
      "\n",
      "Confussion matrix W2V:\n",
      " [[158  42]\n",
      " [ 49 151]]\n",
      "\n",
      "Iteración numero 4:\n",
      "\n",
      "Training Set Accuracy BOW: 1.0\n",
      "Testing Set Accuracy BOW: 0.75\n",
      "Training Set Accuracy W2V: 0.800625\n",
      "Testing Set Accuracy W2V: 0.5725\n",
      "\n",
      "\n",
      "Confussion matrix BOW:\n",
      " [[300 100]\n",
      " [  0   0]]\n",
      "\n",
      "Confussion matrix W2V:\n",
      " [[229 171]\n",
      " [  0   0]]\n",
      "\n",
      "Iteración numero 5:\n",
      "\n",
      "Training Set Accuracy BOW: 1.0\n",
      "Testing Set Accuracy BOW: 0.7875\n",
      "Training Set Accuracy W2V: 0.798125\n",
      "Testing Set Accuracy W2V: 0.66\n",
      "\n",
      "\n",
      "Confussion matrix BOW:\n",
      " [[315  85]\n",
      " [  0   0]]\n",
      "\n",
      "Confussion matrix W2V:\n",
      " [[264 136]\n",
      " [  0   0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5)\n",
    "folds = []\n",
    "for i in range(2000):\n",
    "    folds.append(i)\n",
    "iterador = 1\n",
    "\n",
    "for train, test in kf.split(folds):\n",
    "    print(\"Iteración numero %i:\" % (iterador))\n",
    "    print(\"\")\n",
    "    iterador+=1\n",
    "    lab_fold_train_bow = []\n",
    "    lab_fold_train_w2v = []\n",
    "    lab_fold_test_bow = []\n",
    "    lab_fold_test_w2v = []\n",
    "    \n",
    "    con_fold_train_bow = []\n",
    "    con_fold_train_w2v = []\n",
    "    con_fold_test_bow = []\n",
    "    con_fold_test_w2v = []\n",
    "    for i in train:\n",
    "        lab_fold_train_bow.append(bow_cods[i][1])\n",
    "        lab_fold_train_w2v.append(w2v_cods[i][1])\n",
    "        con_fold_train_bow.append(bow_cods[i][0])\n",
    "        con_fold_train_w2v.append(w2v_cods[i][0])\n",
    "\n",
    "    for j in test:\n",
    "        lab_fold_test_bow.append(bow_cods[j][1])\n",
    "        lab_fold_test_w2v.append(w2v_cods[j][1])\n",
    "        con_fold_test_bow.append(bow_cods[j][0])\n",
    "        con_fold_test_w2v.append(w2v_cods[j][0])\n",
    "        \n",
    "    svm_bow = LinearSVC() \n",
    "    svm_w2v = LinearSVC()\n",
    "    \n",
    "    svm_bow.fit(con_fold_train_bow, lab_fold_train_bow) \n",
    "    svm_w2v.fit(con_fold_train_w2v, lab_fold_train_w2v) \n",
    "    \n",
    "    print(\"Training Set Accuracy BOW:\", svm_bow.score(con_fold_train_bow, lab_fold_train_bow))\n",
    "    print(\"Testing Set Accuracy BOW:\", svm_bow.score(con_fold_test_bow, lab_fold_test_bow))\n",
    "    print(\"Training Set Accuracy W2V:\", svm_w2v.score(con_fold_train_w2v, lab_fold_train_w2v))\n",
    "    print(\"Testing Set Accuracy W2V:\", svm_w2v.score(con_fold_test_w2v, lab_fold_test_w2v))\n",
    "    print(\"\")\n",
    "    print ('\\nConfussion matrix BOW:\\n',confusion_matrix(lab_fold_test_bow, svm_bow.predict(con_fold_test_bow)))\n",
    "    print ('\\nConfussion matrix W2V:\\n',confusion_matrix(lab_fold_test_w2v, svm_w2v.predict(con_fold_test_w2v)))\n",
    "    print(\"\")\n",
    "\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
